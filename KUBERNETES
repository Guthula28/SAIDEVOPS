K8S:

LIMITATIONS OF DOCKER SWARM:
1. CANT DO AUTO-SCALING AUTOMATICALLY
2. CANT DO LOAD BALANCING AUTOMATICALLY
3. CANT HAVE DEFAULT DASHBOARD
4. WE CANT PLACE CONATINER ON REQUITED SERVER.
5. USED FOR EASY APPS. 

HISTORY:
Initially Google created an internal system called Borg (later called as omega) to manage its thousands of applications later they donated the borg system to cncf and they make it as open source. 
initial name is Borg but later cncf rename it to Kubernetes 
the word kubernetes originated from Greek word called pilot or Hailsmen.
Borg: 2014
K8s first version came in 2015.


INTRO:

IT is an open-source container orchestration platform.
It is used to automates many of the manual processes like deploying, managing, and scaling containerized applications.
Kubernetes was developed by GOOGLE using GO Language.
MEM -- > GOOGLE -- > CLUSTER -- > MULTIPLE APPS OF GOOGLE -- > BORG -- > 
Google donated Borg to CNCF in 2014.
1st version was released in 2015.


ARCHITECTURE:

DOCKER : CNCA
K8S: CNPCA

C : CLUSTER
N : NODE
P : POD
C : CONTAINER
A : APPLICATION

SHORTCUT:
CHIRU      : CLUSTER
NAGA       : NODE
PAWAN      : POD
CHARAN     : CONATIAINER
ALLU       : APP

COMPONENTS:
MASTER:

1. API SERVER: communicate with user (takes command execute & give op)
2. ETCD: database of cluster (stores complete info of a cluster ON KEY-VALUE pair)
3. SCHEDULER: select the worker node to schedule pods (depends on hw of node)
4. CONTROLLER: control the k8s objects (n/w, service, Node)

WORKER:

1. KUBELET : its an agent (it will inform all activites to master)
2. KUBEPROXY: it deals with nlw (ip, networks, ports)
3. POD: group of conatiners (inside pod we have app)

Note: all components of a cluster will be created as a pod.


CLUSTER TYPES:

1. SELF MANAGED: WE NEED TO CREATE & MANAGE THEM

minikube = single node cluster
kubeadm = multi node cluster (manual)
kops = multi-node cluster (automation)

2. CLOUD BASED: CLOUD PROVIDERS WILL MANAGE THEM

AWS = EKS = ELASTIC KUBERNETES SERVICE
AZURE = AKS = AZURE KUBERENETS SERVICE
GOOGLE = GKS = GOOGLE KUBERENETS SERVICE



MINIKUBE:
It is a tool used to setup single node cluster on K8's. 
It contains API Servers, ETDC database and container runtime
It is used for development, testing, and experimentation purposes on local. 
Here Master and worker runs on same machine
It is a platform Independent.
Installing Minikube is simple compared to other tools.

NOTE: But we dont implement this in real-time Prod

REQUIREMENTS:

2 CPUs or more
2GB of free memory
20GB of free disk space
Internet connection
Container or virtual machine manager, such as: Docker.

Kubectl is the command line tool for k8s
if we want to execute commands we need to use kubectl.

SETUP:
sudo apt update -y
sudo apt upgrade -y
sudo apt install curl wget apt-transport-https -y
sudo curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
sudo mv minikube-linux-amd64 /usr/local/bin/minikube
sudo chmod +x /usr/local/bin/minikube
sudo minikube version
sudo curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo curl -LO "https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256"
sudo echo "$(cat kubectl.sha256) kubectl" | sha256sum --check
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
sudo minikube start --driver=docker --force

NOTE: When you download a command as binary file it need to be on /usr/local/bin 
because all the commands in linux will be on /usr/local/bin 
and need to give executable permission for that binary file to work as a  command.



POD:
It is a smallest unit of deployment in K8's.
It is a group of containers.
Pods are ephemeral (short living objects)
Mostly we can use single container inside a pod but if we required, we can create multiple containers inside a same pod.
when we create a pod, containers inside pods can share the same network namespace, and can share the same storage volumes .
While creating pod, we must specify the image, along with any necessary configuration and resource limits.
K8's cannot communicate with containers, they can communicate with only pods.
 
We can create this pod in two ways, 
1. Imperative(command) 
2. Declarative (Manifest file)


IMPERATIVE:

kubectl run pod1 --image radhikasn458/paytmmovies:latest
kubectl get pods/pod/po
kubectl get pod -o wide
kubectl describe pod pod1
kubectl delete pod pod1

DECRALATIVE: by using file called manifest file

MANDATORY FEILDS: without these fields we cant create manifest

apiVersion:
kind:
metadata:
spec:


vim pod.yml

apiVersion: v1
kind: Pod
metadata:
  name: pod1
spec:
  containers:
    - image: vinodvanama/paytmtrain:latest
      name: cont1

execution: 
kubectl create -f pod.yml
kubectl get pods/pod/po
kubectl get pod -o wide
kubectl describe pod pod1
kubectl delete -f raham.yml

DRAWBACK: once pod is deleted we can't retive the pod.

HISTORY:
    1  vim minikube.sh
    2  sh minikube.sh
    3  kubectl get pods
    4  kubernetes
    5  kubectl run pod1 --image radhikasn458/paytmmovies:latest
    6  kubectl get pods
    7  kubectl get pod
    8  kubectl get po
    9  kubectl get po -o wide
   10  kubectl describe pod pod1
   11  kubectl delete pod pod1
   12  kubectl run raham --image radhikasn458/paytmmovies:latest
   13  kubectl get po
   14  kubectl get po -o wide
   15  kubectl describe pod raham
   16  kubectl delete pod raham
   17  vim raham.yml
   18  kubectl create -f raham.yml
   19  kubectl get po
   20  kubectl get po -o wide
   21  kubectl describe po
   22  kubectl delete po abc
   23  history


-------------------------------------------------------------------------------------------

REPLICASET:
rs -- > pods
it will create multiple copies of same pod.
if we delete one pod automatically it will create new pod.
All the pods will have same config.
only pod names will be differnet.


LABLES: individual pods are difficult to manage 
so we give a common label to group them and work with them together
SELECTOR: Used to select pods with same labels.


use kubectl api-resources for checking the objects info

vim replicaset.yml

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: movies
  labels:
    app: paytm
spec:
  replicas: 3
  selector:
    matchLabels:
      app: paytm
  template:
    metadata:
      labels:
        app: paytm
    spec:
      containers:
        - name: cont1
          image: radhkasn458/paytmmovies:latest




To list rs		:kubectl get rs/replicaset
To show addtional info	:kubectl get rs -o wide
To show complete info	:kubectl describe rs name-of-rs
To delete the rs	:kubectl delete rs name-of-rs
to get lables of pods 	: kubectl get pods -l app=paytm
TO scale rs		: kubectl scale rs/movies --replicas=10 (LIFO)


LIFO: LAST IN FIRST OUT.
IF A POD IS CREATED LASTLY IT WILL DELETE FIRST WHEN SCALE OUT.

ADV:
Self healing
scaling

DRAWBACKS:
1. we cant rollin and rollout, we cant update the application in rs.

DEPLOYMENT:
deploy -- > rs -- > pods
we can update the application.
its high level k8s objects.

vim deploy.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: movies
  labels:
    app: paytm
spec:
  replicas: 3
  selector:
    matchLabels:
      app: paytm
  template:
    metadata:
      labels:
        app: paytm
    spec:
      containers:
        - name: cont1
          image: radhikasn458/movies:latest


To list deployment	:kubectl get deploy
To show addtional info	:kubectl get deploy -o wide
To show complete info	:kubectl describe deploy name-of-deployment
To delete the deploy	:kubectl delete deploy name-of-deploy
to get lables of pods 	:kubectl get pods -l app=paytm
TO scale deploy		:kubectl scale deploy/name-of-deploy --replicas=10 (LIFO)
To edit deploy		:kubectl edit deploy/name-of-deploy
to show all pod labels	:kubectl get pods --show-labels
To delete all pods	:kubectl delete pod --all

kubectl rollout history deploy/movies
kubectl rollout undo deploy/movies
kubectl rollout status deploy/movies
kubectl rollout pause deploy/movies
kubectl rollout resume deploy/movies


KUBECOLOR:
wget https://github.com/hidetatz/kubecolor/releases/download/v0.0.25/kubecolor_0.0.25_Linux_x86_64.tar.gz
tar -zxvf kubecolor_0.0.25_Linux_x86_64.tar.gz
./kubecolor
chmod +x kubecolor
mv kubecolor /usr/local/bin/
kubecolor get po

HISTORY:

    1  vim mk.sh
    2  sh mk.sh
    3  kubectl  get po
    4  kubectl  get po -A
    5  vim raham.yml
    6  kubectl create -f raham.yml
    7  kubectl get po
    8  kubectl get po -o wide
    9  kubectl describe po raham
   10  kubectl delete po raham
   11  kubectl api-resources
   12  vim raham.yml
   13  kubectl create -f raham.yml
   14  kubectl get replicaset
   15  kubectl get rs
   16  kubectl get rs -o wide
   17  kubectl describe rs movies
   18  kubectl get po
   19  kubectl delete rs movies
   20  vim raham.yml
   21  kubectl create -f raham.yml
   22  kubectl get rs
   23  kubectl get po
   24  kubectl get po --show-lables
   25  kubectl get po --show-labels
   26  kubectl delete po movies-kgd2p
   27  kubectl get po
   28  kubectl delete po movies-xj96q
   29  kubectl get po
   30  kubectl scale rs/movies --replicas=10
   31  kubectl get po
   32  kubectl scale rs/movies --replicas=3
   33  kubectl get po
   34  kubectl describe pod -l app=paytm
   35  kubectl describe pod
   36  kubectl describe pod | grep -i image
   37  kubectl describe pod | grep -i image:
   38  kubectl edit rs/movies
   39  kubectl describe pod | grep -i image:
   40  kubectl delete -f raham.yml
   41  vim raham.yml
   42  kubectl create -f raham.yml
   43  kubectl get deployment
   44  kubectl get deploy
   45  kubectl get deploy -o wide
   46  kubectl describe deploy movie
   47  kubectl describe po | grep -i image:
   48  kubectl edit deploy/movies
   49  kubectl describe po | grep -i image:
   50  kubectl get po
   51  kubectl describe po | grep -i image:
   52  kubectl rollout history deploy/movies
   53  kubectl rollout undo deploy/movies
   54  kubectl describ po | grep -i image:
   55  kubectl describe po | grep -i image:
   56  kubectl rollout history deploy/movies
   57  kubectl rollout undo deploy/movies
   58  kubectl describe po | grep -i image:
   59  kubectl rollout history deploy/movies
   60  kubectl rollout status deploy/movies
   61  kubectl rollout pause deploy/movies
   62  kubectl rollout undo deploy/movies
   63  kubectl rollout resume deploy/movies
   64  kubectl rollout undo deploy/movies
   65  kubectl get po
   66  wget https://github.com/hidetatz/kubecolor/releases/download/v0.0.25/kubecolor_0.0.25                                                                                                                                                                                                                                                                                    _Linux_x86_64.tar.gz
   67  tar -zxvf kubecolor_0.0.25_Linux_x86_64.tar.gz
   68  ./kubecolor
   69  chmod +x kubecolor
   70  mv kubecolor /usr/local/bin/
   71  kubecolor get po
   72  kubectl get po
   73  kubecolor get po -o wide
   74  history

===================================================================================================

KOPS:
INFRASTRUCTURE: Resources used to run our application on cloud.
EX: Ec2, VPC, ALB, -------------


Minikube -- > single node cluster
All the pods on single node 
if that node got deleted then all pods will be gone.

KOPS:
kOps, also known as Kubernetes operations.
it is an open-source tool that helps you create, destroy, upgrade, and maintain a highly available, production-grade Kubernetes cluster. 
Depending on the requirement, kOps can also provide cloud infrastructure.
kOps is mostly used in deploying AWS and GCE Kubernetes clusters. 
But officially, the tool only supports AWS. Support for other cloud providers (such as DigitalOcean, GCP, and OpenStack) are in the beta stage.


ADVANTAGES:
•	Automates the provisioning of AWS and GCE Kubernetes clusters
•	Deploys highly available Kubernetes masters
•	Supports rolling cluster updates
•	Autocompletion of commands in the command line
•	Generates Terraform and CloudFormation configurations
•	Manages cluster add-ons.
•	Supports state-sync model for dry-runs and automatic idempotency
•	Creates instance groups to support heterogeneous clusters

ALTERNATIVES:
Amazon EKS , MINIKUBE, KUBEADM, RANCHER, TERRAFORM.


STEP-1: GIVING PERMISSIONS 

KOps Is a third party tool if it want to create infrastructure on aws 
aws need to give permission for it so we can use IAM user to allocate permission for the kops tool

IAM -- > USER -- > CREATE USER -- > NAME: KOPS -- > Attach Polocies Directly -- > AdministratorAccess -- > NEXT -- > CREATE USER
USER -- > SECURTITY CREDENTIALS -- > CREATE ACCESS KEYS -- > CLI -- > CHECKBOX -- >  CREATE ACCESS KEYS -- > DOWNLOAD 

aws configure (run this command on server)

SETP-2: INSTALL KUBECTL AND KOPS

curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
wget https://github.com/kubernetes/kops/releases/download/v1.25.0/kops-linux-amd64
chmod +x kops-linux-amd64 kubectl
mv kubectl /usr/local/bin/kubectl
mv kops-linux-amd64 /usr/local/bin/kops

vim .bashrc
export PATH=$PATH:/usr/local/bin/  -- > save and exit
source .bashrc

SETP-3: CREATING BUCKET 
aws s3api create-bucket --bucket devopsbatchjan2024pm.k8s.local --region us-east-1
aws s3api put-bucket-versioning --bucket devopsbatchjan2024pm.k8s.local --region us-east-1 --versioning-configuration Status=Enabled
export KOPS_STATE_STORE=s3://devopsbatchjan2024pm.k8s.local

SETP-4: CREATING THE CLUSTER
kops create cluster --name rahams.k8s.local --zones us-east-1a --master-count=1 --master-size t2.medium --node-count=2 --node-size t2.micro
kops update cluster --name rahams.k8s.local --yes --admin


Suggestions:
 * list clusters with: kops get cluster
 * edit this cluster with: kops edit cluster rahams.k8s.local
 * edit your node instance group: kops edit ig --name=rahams.k8s.local nodes-us-east-1a
 * edit your master instance group: kops edit ig --name=rahams.k8s.local master-us-east-1a


ADMIN ACTIVITIES:
To scale the worker nodes:
kops edit ig --name=rahams.k8s.local nodes-us-east-1a
kops update cluster --name rahams.k8s.local --yes --admin 
kops rolling-update cluster --yes

ADMIN ACTIVITIES:
kops update cluster --name rahams.k8s.local --yes
kops rolling-update cluster

NOTE: In real time we use fine node cluster two master nodes and three worker nodes.

NOTE: its My humble request for all of you not to delete the cluster manually and do not delete any server use the below command to delete the cluster.

TO DELETE: kops delete cluster --name rahams.k8s.local --yes

NAMESPACES:

NAMESPACE: It is used to divide the cluster to multiple teams on real time.
it is used to isolate the env.

CLUSTER: HOUSE
NAMESPACES: ROOM

Each namespace is isolated.
if your are room-1 you are not able to see room-2.
If dev team create a pod on dev ns testing team cant able to access it.
we cant access the objects from one namespace to another namespace.


TYPES:

default           : Is the default namespace, all objects will create here only
kube-node-lease   : it will store object which is taken from one namespace to another.
kube-public	  : all the public objects will store here.      
kube-system 	  : default k8s will create some objects, those are storing on this ns.

NOTE: Every component of Kubernetes cluster is going to create in the form of pod
And all these pods are going to store on kUBE-SYSTEM ns.

kubectl get pod -n kube-system	: to list all pods in kube-system namespace
kubectl get pod -n default	: to list all pods in default namespace
kubectl get pod -n kube-public	: to list all pods in kube-public namespace
kubectl get po -A		: to list all pods in all namespaces
kubectl get po --all-namespaces

kubectl create ns dev	: to create namespace
kubectl config set-context --current --namespace=dev : to switch to the namespace
kubectl config view --minify | grep namespace : to see current namespace
kubectl run dev1 --image nginx
kubectl run dev2 --image nginx
kubectl run dev3 --image nginx
kubectl create ns test	: to create namespace
kubectl config set-context --current --namespace=test : to switch to the namespace
kubectl config view --minify | grep namespace : to see current namespace
kubectl get po -n dev
kubectl delete pod dev1 -n dev
kubectl delete ns dev	: to delete namespace
kubectl delete pod --all: to delete all pods


NOTE: By deleting  the ns all objects also gets deleted.
in real time we use rbac concept to restrict the access from one namespace to another.
so users cant access/delete ns, because of the restriction we provide.
we create roles and rolebind for the users.

=============================================================================

SERVICE: It is used to expose the application in k8s.

TYPES:
1. CLUSTERIP: It will work inside the cluster.
it will not expose to outer world.

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: movies
  name: movies-deploy
spec:
  replicas: 10
  selector:
    matchLabels:
      app: movies
  template:
    metadata:
      labels:
        app: movies
    spec:
      containers:
      - name: cont1
        image: rahamshaik/moviespaytm:latest
        ports:
          - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: service1
spec:
  type: ClusterIP
  selector:
    app: movies
  ports:
    - port: 80

DRAWBACK:
We cannot use app outside.

2. NODEPORT: It will expose our application in a particular port.
Range: 30000 - 32767 (in sg we need to give all traffic)
if we dont sepcify k8s service will take random port number.

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: movies
  name: movies-deploy
spec:
  replicas: 10
  selector:
    matchLabels:
      app: movies
  template:
    metadata:
      labels:
        app: movies
    spec:
      containers:
      - name: cont1
        image: rahamshaik/moviespaytm:latest
        ports:
          - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: service1
spec:
  type: NodePort
  selector:
    app: movies
  ports:
    - port: 80
      nodePort: 31111

NOTE: UPDATE THE SG (REMOVE OLD TRAFFIC AND GIVE ALL TRAFFIC)
DRAWBACK:
EXPOSING PUBLIC-IP & PORT 
PORT RESTRICTION.

3. LOADBALACER: It will expose our app and distribute load blw pods.
it will expose the application with dns [Domain Name System] -- > 53
to crete dns we use Route53 

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: swiggy
  name: swiggy-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: swiggy
  template:
    metadata:
      labels:
        app: swiggy
    spec:
      containers:
      - name: cont1
        image: rahamshaik/trainservice:latest
        ports:
          - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: abc
spec:
  type: LoadBalancer
  selector:
    app: swiggy
  ports:
    - port: 80
      targetPort: 80

==============================================================


METRIC SERVER:
if we install metric server in k8s cluster it can collects metrics like cpu, ram -- from all the pods and nodes in cluster.
we can use kubectl top po/no to see metrics


Metrics Server offers:

    A single deployment that works on most clusters (see Requirements)
    Fast autoscaling, collecting metrics every 15 seconds.
    Resource efficiency, using 1 milli core of CPU and 2 MB of memory for each node in a cluster.
    Scalable support up to 5,000 node clusters.


You can use Metrics Server for:
CPU/Memory based horizontal autoscaling (Horizontal Autoscaling)
Automatically adjusting/suggesting resources needed by containers (Vertical Autoscaling)


Horizontal: New 
Vertical: Existing



In Kubernetes, a HorizontalPodAutoscaler automatically updates a workload resource (such as a Deployment or ReplicaSet), with the aim of automatically scaling the workload to match demand.

Example : if you have pod-1 with 50% load and pod2 with 50% load then average will be (50+50/2=50) average value is 50
but if pod-1 is exceeding 60% and pod-2 50% then average will be 55% (then here we need to create a pod-3 becaue its exceeding the average)

Here we need to use metric server whose work is to collect the metrics (cpu & mem info)
metrics server is connected to the HPA and give information to HPA 
Now HPA will analysis metrics for every 30 sec and create a new pod if needed.


COOLING PERIOD:


scaling can be done only for scalabel objects (ex: RS, Deployment, RC )
HPA is implemented as a K8S API Resources and a controller.
Controller Periodically adjust the number of replicas in RS, RC and Deployment depends on average.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: movies
  labels:
    app: movies
spec:
  replicas: 3
  selector:
    matchLabels:
      app: movies
  template:
    metadata:
      labels:
        app: movies
    spec:
      containers:
        - name: cont1
          image: yashuyadav6339/movies:latest


kubectl apply -f hpa.yml
kubectl get all
kubectl get deploy 
kubectl autoscale deployment movies --cpu-percent=20 --min=1 --max=10
kubectl get hpa
kubectl desribe hpa movies
kubectl get al1

open second termina and give
kubectl get po --watch

come to first terminal and go inside pod
kubectl exec mydeploy-6bd88977d5-7s6t8 -it -- /bin/bash

apt update -y
apt install stress -y
stress 

check terminal two to see live pods


DAEMONSET: used to create one pod on each workernode.
Its the old version of Deployment.
if we create a new node a pod will be automatically created.
if we delete a old node a pod will be automatically removed.
daemonsets will not be removed at any case in real time.
Usecases: we can create pods for Logging, Monitoring of nodes 

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: movies
  labels:
    app: movies
spec:
  selector:
    matchLabels:
      app: movies
  template:
    metadata:
      labels:
        app: movies
    spec:
      containers:
        - name: cont1
          image: yashuyadav6339/movies:latest


Replication controller is the previous version of replicaset.

RC: uses equity based selector (=, =!)
RS: used set based selector ( env in (dev, test) [in, notin]


apiVersion: v1
kind: ReplicationController
metadata:
  name: nginx
spec:
  replicas: 3
  selector:
    app: nginx
  template:
    metadata:
      name: nginx
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: rahamshaik/moviespaytm:latest
        ports:
        - containerPort: 80
QUOTAS:
k8s cluster can be divied into namespaces
By default the pod in K8s will run with no limitations of Memory and CPU
But we need to give the limit for the Pod 
It can limit the objects that can be created in a namespace and total amount of resources.
when we create a pod scheduler will the limits of node to deploy pod on it.
here we can set limits to CPU, Memory and Storage
here CPU is measured on cores and memory in bytes.
1 cpu = 1000 millicpus  ( half cpu = 500 millicpus (or) 0.5 cpu)

Here Request means how many we want
Limit means how many we can create maximum

limit can be given to pods as well as nodes
the default limit is 0

if you mention request and  limit then everything is fine
if you dont mention request and mention limit then Request=Limit
if you mention request and not mention limit then Request=!Limit

IMPORTANT:
Ever Pod in namespace must have CPU limts.
The amount of CPU used by all pods inside namespace must not exceed specified limit.

DEFAULT RANGE:
CPU : 
MIN		= REQUEST = 0.5
MAX		= LIMIT = 1

MEMORY :
MIN	= REQUEST = 500M
MAX	= LIMIT = 1G


kubectl create ns dev
kubectl config set-context $(kubectl config current-context) --namespace=dev

vim dev-quota.yml

apiVersion: v1
kind: ResourceQuota
metadata:
  name: dev-quota
  namespace: dev
spec:
  hard:
    pods: "5"
    limits.cpu: "1"
    limits.memory: 1Gi

kubectl create -f dev-quota.yml
kubectl get quota


EX-1: Mentioning Limits  = SAFE WAY

apiVersion: apps/v1
kind: Deployment
metadata:
  name: movies
  labels:
    app: movies
spec:
  replicas: 3
  selector:
    matchLabels:
      app: movies
  template:
    metadata:
      labels:
        app: movies
    spec:
      containers:
        - name: cont1
          image: yashuyadav6339/movies:latest
          resources:
            limits:
              cpu: "1"
              memory: 512Mi

kubectl create -f dep.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: movies
  labels:
    app: movies
spec:
  replicas: 3
  selector:
    matchLabels:
      app: movies
  template:
    metadata:
      labels:
        app: movies
    spec:
      containers:
        - name: cont1
          image: yashuyadav6339/movies:latest
          resources:
            limits:
              cpu: "0.2"
              memory: 100Mi

kubectl create -f dep.yml



EX-2: MENTION LIMITS & REQUESTS = SAFE WAY


apiVersion: apps/v1
kind: Deployment
metadata:
  name: movies
  labels:
    app: movies
spec:
  replicas: 3
  selector:
    matchLabels:
      app: movies
  template:
    metadata:
      labels:
        app: movies
    spec:
      containers:
        - name: cont1
          image: yashuyadav6339/movies:latest
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: "0.2"
              memory: 100Mi

EX-3: MENTION only REQUESTS =  NOT SAFE WAY


apiVersion: apps/v1
kind: Deployment
metadata:
  name: movies
  labels:
    app: movies
spec:
  replicas: 3
  selector:
    matchLabels:
      app: movies
  template:
    metadata:
      labels:
        app: movies
    spec:
      containers:
        - name: cont1
          image: yashuyadav6339/movies:latest
          resources:
            requests:
              cpu: "0.2"
              memory: 100Mi

===================================================================================================================

RBAC:

role : set of permissions for one ns
role binding: adding users to role
these will work on single namespace

cluster role: set of permissions for entire ns
cluster role binding: adding users to cluster role
these will work on all namespaces


when we run kubectl get po k8s api server will authenticate and check authorization
authentication: permission to login
authorization: permission to work on resources

To authenticate API requests, k8s uses the following options: 
client certificates, 
bearer tokens, 
authenticating proxy, 
or HTTP basic auth.

Kubernetes doesn’t have an API for creating users. 
Though, it can authenticate and authorize users.

We will choose the client certificates as it is the simplest among the four options.

why certs needed on k8s: for authentication purpose.
certs will have users & keys for login. 

1. Create a client certificate
We’ll be creating a key and certificate sign request (CSR) needed to create the certificate. Let’s create a directory where to save the certificates. I’ll call it cert:

mkdir dev1 && cd dev1

1. Generate a key using OpenSSL: 
openssl genrsa -out dev1.key 2048

2. Generate a Client Sign Request (CSR) : 

openssl req -new -key dev1.key -out dev1.csr -subj "/CN=dev1/O=group1"
ls ~/.minikube/ 

3. Generate the certificate (CRT): 

openssl x509 -req -in dev1.csr -CA ~/.minikube/ca.crt -CAkey ~/.minikube/ca.key -CAcreateserial -out dev1.crt -days 500

Now, that we have the .key and the .crt, we can create a user.



Create a user
1. Set a user entry in kubeconfig
kubectl config set-credentials dev1 --client-certificate=dev1.crt --client-key=dev1.key

2.Set a context entry in kubeconfig
kubectl config set-context dev1-context --cluster=minikube --user=dev1
kubectl config view

3.Switching to the created user
kubectl config use-context dev1-context
$ kubectl config current-context # check the current context
dev1-context
But, now, dev1 doesn’t have any access privileges to the cluster. For that we’ll have access denied if we try to create any resource:

$ kubectl create namespace ns-test
Error from server (Forbidden): namespaces is forbidden: User "dev1" cannot create resource "namespaces" in API group "" at the cluster scope
3. Grant access to the user
To give access to manage k8s resources to dev1, we need to create a Role and a BindingRole.

kubectl config use-context minikube
kubectl create ns dev



3.1. Create a Role

Let’s create a Role object in role.yaml file. The Role will define which resources could be accessed and which operations/verbs could be used.

kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: dev
  name: pod-reader
rules:
- apiGroups: [""] # "" indicates the core API group
  resources: ["pods"]
  verbs: ["get", "watch", "list"]

Resources: pod, deployment, namespace, secret, configmap, service, persistentvolume…

Verbs: get, list, watch, create, delete, update, edit, exec.

3.2. Create a BindingRole

We want to match the dev1 to the role created above named : pod-reader. To do that, we need to use RoleBinding.

kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: read-pods
  namespace: dev
subjects:
- kind: User
  name: dev1 # Name is case sensitive
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role #this must be Role or ClusterRole
  name: pod-reader # must match the name of the Role
  apiGroup: rbac.authorization.k8s.io

Deploy both the role.yaml and role-binding.yaml to k8s, and don’t forget to use the context of minikube:

kubectl config set-context --current --namespace=dev

$ kubectl config use-context minikube
Switched to context "minikube".
$ kubectl apply -f role.yaml
role.rbac.authorization.k8s.io/pod-reader created
$ kubectl apply -f role-binding.yaml
rolebinding.rbac.authorization.k8s.io/read-pods created
We check that the Role and BindingRole was created successfully:

$ kubectl get roles
NAME       AGE
pod-reader 2m
$ kubectl get rolebindings
NAME        AGE
read-pods   2m

We used Role to scope the rules to only one namespace, but we can also use ClusterRole to define more than one namespace. RoleBinding is used to bind to Role and ClusterRoleBinding is used to bind to ClusterRole.

4. Testing the allowed operations for user
Switch again to dev1 and try one of the non allowed operations like to create a namespace. This will fail, because dev1 is not allowed to do so.

$ kubectl config use-context dev1-context
$ kubectl create namespace ns-test 
$ kubectl get pods 

NOTE: TO GIVE PERMISSION FOR ROLE SWITCH TO MINIKUBE CONTEXT
TO CHECK THE GIVEN PERMISSION FOR ROLE SWITCH TO dev1 CONTEXT



EXAMPLES:
EX-1:
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: ["*"] # "" indicates the core API group
  resources: ["pods"]
  verbs: ["get", "watch", "list", "create", "delete"]


EX-2:

kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: ["*"] # "" indicates the core API group
  resources: ["pods", "deployments"]
  verbs: ["get", "watch", "list", "create", "delete"]

EX-3:
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: ["*"] # "" indicates the core API group
  resources: ["pods", "deployments", "pods/log"]
  verbs: ["get", "watch", "list", "create", "delete"]


ADMIN:

kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: admin
rules:
- apiGroups: ["*"] # "" indicates the core API group
  resources: ["*"]
  verbs: ["*"]


kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: adminbind
  namespace: default
subjects:
- kind: User
  name: user2 # Name is case sensitive
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role #this must be Role or ClusterRole
  name: admin # must match the name of the Role
  apiGroup: rbac.authorization.k8s.io


kubectl create deploy raham --image nginx
kubectl create cm one --from-literal=app=paytm
kubectl create ns test
kubectl logs pod1

======================================================================

VOLUMES:

Basically these K8's will works on short living data. So lets unveil the power of volumes like EmptyDir, HostPath, PV & PVC.

The data is a very important thing for an application. In K8's, data is kept for a short time in the applications in the pods/containers. By default the data will no longer available. To overcome this we will use Kubernetes Volumes.

But before going into the types of Volumes. Let’s understand some facts about pods and containers' short live data.


Types of volumes:
 EmptyDir
 HostPath
 Persistent Volume
 Persistent Volume Claim(PVC)


EMPTY DIR:
This volume is used to share the data between multiple containers within a pod instead of the host machine or any Master/Worker Node.
EmptyDir volume is created when the pod is created and it exists as long as a pod.
There is no data available in the EmptyDir volume type when it is created for the first.
Containers within the pod can access the other containers' data. However, the mount path can be different for each container.
If the Containers get crashed then, the data will still persist and can be accessible by other or newly created containers.


HOSTPATH:
This volume type is the advanced version of the previous volume type EmptyDir.
In EmptyDir, the data is stored in the volumes that reside inside the Pods only where the host machine doesn’t have the data of the pods and containers.
hostpath volume type helps to access the data of the pods or container volumes from the host machine.
hostpath replicates the data of the volumes on the host machine and if you make the changes from the host machine then the changes will be reflected to the pods volumes(if attached).


PV:
Persistent means always available.
Persistent Volume is an advanced version of EmptyDir and hostPath volume types.
Persistent Volume does not store the data over the local server. It stores the data on the cloud or some other place where the data is highly available.
In previous volume types, if pods get deleted then the data will be deleted as well. But with the help of Persistent Volume, the data can be shared with other pods or other worker node’s pods as well after the deletion of pods.
PVs are independent of the pod lifecycle, which means they can exist even if no pod is using them.
With the help of Persistent Volume, the data will be stored on a central location such as EBS, Azure Disks, etc.
One Persistent Volume is distributed across the entire Kubernetes Cluster. So that, any node or any node’s pod can access the data from the volume accordingly.
In K8S, a PV is a piece of storage in the cluster that has been provisioned by an administrator.
If you want to use Persistent Volume, then you have to claim that volume with the help of the manifest YAML file.
When a pod requests storage via a PVC, K8S will search for a suitable PV to satisfy the request. 
If a PV is found that matches the request, the PV is bound to the PVC and the pod can use the storage. 
If no suitable PV is found, K8S then PVC will remain unbound (pending).


PVC:
To get the Persistent Volume, you have to claim the volume with the help of PVC.
When you create a PVC, Kubernetes finds the suitable PV to bind them together.
After a successful bound to the pod, you can mount it as a volume.
Once a user finishes its work, then the attached volume gets released and will be used for recycling such as new pod creation for future usage.
If the pod is terminating due to some issue, the PV will be released but as you know the new pod will be created quickly then the same PV will be attached to the newly created Pod.
After bounding is done to pod you can mount it as a volume. The pod specifies the amount and type of storage it needs, and the cluster provisions a persistent volume that matches the request. If its not matches then it will be in pending state. 


EBS:
Now, As you know the Persistent Volume will be on Cloud. So, there are some facts and terms and conditions are there for EBS because we are using AWS cloud for our K8 learning. So, let’s discuss it as well:

EBS Volumes keeps the data forever where the emptydir volume did not. If the pods get deleted then, the data will still exist in the EBS volume.
The nodes on which running pods must be on AWS Cloud only(EC2 Instances).
Both(EBS Volume & EC2 Instances) must be in the same region and availability zone.
EBS only supports a single EC2 instance mounting a volume



CODES:



cat pv.yml 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: example-pv
spec:
  capacity:
    storage: 20Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  awsElasticBlockStore:
    volumeID: vol-07b55f3d3b90c5fa7
    fsType: ext4


PVC.YML

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: example-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi

DEP.YML

apiVersion: apps/v1
kind: Deployment
metadata:
  name: pvdeploy
spec:
  replicas: 1
  selector:
    matchLabels:
     app: swiggy
  template:
    metadata:
      labels:
        app: swiggy
    spec:
      containers:
      - name: raham
        image: centos
        command: ["bin/bash", "-c", "sleep 10000"]
        volumeMounts:
        - name: example-pv
          mountPath: "/tmp/persistent"
      volumes:
        - name: example-pv
          persistentVolumeClaim:
            claimName: example-pvc

TROUBESHOOT:

There are several pod-level issues that can occur in Kubernetes (K8s). Here are some common pod-level issues and their possible causes:

CreateContainerConfigError
ImagePullBackOff or ErrorImagePull
CrashLoopBackOff

CONFTAINER CONFIG ERROR:
This error is usually the result of a missing Secret or ConfigMap. Secrets are Kubernetes objects used to store sensitive information like database credentials. 
ConfigMaps store data as key-value pairs, and are typically used to hold configuration information used by multiple pods.

kubectl create deploy raham2 --image=mariadb
kubectl get po - o wide
raham2   0/1     Error              3 (37s ago)   73s     10.244.0.9   minikube <none> <none>
you will get error after some time 
kubectl describe pod raham2-66c4648c74-hn45t
now you will gte exit code as one  : that means some issue will be there
kubectl logs raham2-66c4648c74-hn45t
kubectl set env deploy raham2 MARIADB_ROOT_PASSWORD=raham123
kubectl get po - o wide
Now it will run

ImagePullBackOff or ErrorImagePull:

RESON:
Image availability
Image registry authentication
Image registry connectivity  (Firewall rules or network restrictions of Cluster)
Image registry permissions
Image pull rate limits
Network connectivity and DNS resolution 
Proxy configurations
Resource constraints

kubectl run pod1 --image rahamshaik007/paytm movies:latest
kubectl run pod2 --image rahamshaik007/paytmmovies:latest

==============================================================================================
Probes are a Kubernetes feature that allows you to determine the health and readiness of a container running inside a pod. 

Kubernetes provides three types of probes:
Readiness Probe
Liveness Probe
StartUp Probe

Liveness Probe:
It allows you to check the health of a running container in a pod. 
It is used to ensure that the container is running as expected and to restart the container if it becomes unresponsive.
LivenessProbe works by periodically sending a request to a specified endpoint in the container and checking the response. 
If the response indicates that the container is healthy, it continues to run. 
If the response indicates that the container is unhealthy, Kubernetes will restart the container.

There are three types of probes that can be used for liveness checks: HTTP GET, TCP socket, and command execution. 
HTTP GET probes send an HTTP request to a specific endpoint on the container, 
TCP socket probes attempt to open a socket to a specific port on the container, 
and command execution probes execute a command inside the container and check its output.

LivenessProbe is an important feature for ensuring that your application is always running and available to users. 
By configuring it properly, you can avoid downtime caused by unresponsive containers and ensure that your users have a consistent experience with your application.



FailureThreshold: 
specifies the number of times a probe must fail before the kubelet considers the container to have failed.

initialDelaySeconds: 
it is set to 30 sec the kubelet will wait for 30 seconds after the container starts before performing the first liveness probe. 

periodSeconds:
if the periodSeconds of a liveness probe is set to 10, the kubelet will perform a  probe every 10 seconds to check the health of the container.

SuccessThreshold:
For example, if the success threshold of a readiness probe is set to 3, the kubelet will perform a readiness probe every period seconds and if the probe succeeds three times in a row, it will consider the container to be ready to receive traffic

timeoutSeconds:
used to specify the number of seconds the kubelet should wait for a probe to complete before considering it as failed.
For example, if the timeoutSeconds of a liveness probe is set to 5 and the probe takes longer than 5 seconds to complete, the kubelet.

LIVENESS PROBE:
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-exec
spec:
  containers:
  - name: liveness
    image: registry.k8s.io/busybox
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 30
    livenessProbe:
      exec:
        command:
        - ls /tmp
      initialDelaySeconds: 5
      periodSeconds: 5

READYNESS PROBE:


It checks whether a container is ready to serve traffic. 
It is used to ensure that the container has started up successfully and is ready to receive traffic before it is added to the load balancer pool.
it works by periodically sending a request to a specified endpoint in the container and checking the response. 
If the response indicates that the container is ready to receive traffic, it is added to the load balancer pool. 
If the response indicates that the container is not ready to receive traffic, it is not added to the load balancer pool until it becomes ready.
Like LivenessProbe, there are three types of probes that can be used for readiness checks: HTTP GET, TCP socket, and command execution. 

HTTP GET probes send an HTTP request to a specific endpoint on the container, 
TCP socket probes attempt to open a socket to a specific port on the container, 
and command execution probes execute a command inside the container and check its output.



apiVersion: v1
kind: Pod
metadata:
  name: raham-ready
spec:
  containers:
  - name: cont1
    image: nginx
    command:
      - sleep
      - "3600"
    readinessProbe:
      periodSeconds: 10
      exec:
        command:
        - cat
        - /root/app.yml
    resources: {}

kubectl apply -f abc.yml
kubectl get po

here container will be on running but ready will get 0
kubectl describe pod raham-ready
Readiness probe failed: cat: /root/app.yml: No such file or directory

kubectl exec -it raham-ready -- touch /root/app.yml
kubectl get po

now it will come on running state 

Readiness Probes: Used to check if the application is ready to use and serve the traffic
Liveness Probes: Used to check if the container is available and alive.
Startup Probes: Used to takes some time to initialize application 

readiness probe will check something and liveness probe will monitor what readiness probe is checking.

EX: I will create a file in root folder in a pod, then i will tell to Readiness proceed furthur if you have the file only 
The Liveness probe will check the readniness probe is working or not
if its not running liveness probe will stop.


========================================================================================

ENV VARS:

It is a way to pass configuration information to containers running within pods. 
To set Env  vars it include the env or envFrom field in the configuration file.

ENV: To pass variables directly
ENVFROM:
allows you to set environment variables for a container by referencing either a ConfigMap or a Secret. 
When you use envFrom, all the key-value pairs in the referenced ConfigMap or Secret are set as environment variables for the container. 
You can also specify a common prefix string


DEFINING LEVELS:
Container level:  It will be applied to specific container.
Pod level:  It will be applied to all the containers within the pod.
Deployment level: It will be applied to all the pods created by that deployment. This is useful when you want to provide configuration settings that are shared across multiple instances of the same application.

SETTING ENV VARS:
Inside the Pod or Container spec
Using ConfigMaps
Using Secrets


CONFIG MAPS:
It is used to store the data in key-value pair, files, or command-line arguments that can be used by pods, containers and other resources in cluster
But the data should be non confidential data ()
Here we can set the configuration of data of application seperately
It decouple environment specific configuration.
But it does not provider security and encryption.
If we want to provide encryption use secrets in kubernetes.
Limit of config map data in only 1 MB (we cannot store more than that)
But if we want to store a large amount of data in config maps we have to mount a volume or use a seperate database or file service.

USE CASES IN CONFIG MAPS:
Configure application settings: By using this config maps, we can store the configuration data that helps to run our application like database connections and environment variables
Configuring a pod or container: It is used to send a configuration data to a pod or container at runtime like CLI or files.
Sharing configuration data across multiple resources: By using this configuration data multiple resources can access, such as a common configuration file for different pods or services.
We can store the data: By using this config maps, we can store the data like IP address, URL's and DNS etc...



CONFIG MAP FROM  FILE:

cat configmap.yml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-config
data:
  database-url: "http://localhost:5432"
  redis: "http://localhost:6379"

cat pod.yml
apiVersion: v1
kind: Pod
metadata:
  name: pod1
spec:
  containers:
    - image: hpakala53/paytmtrain:latest
      name: cont1
      envFrom:
        - configMapRef:
            name: my-config

CONFIG MAP FROM CLI:

kubectl create cm newconfig1 --from-literal=user=raham

apiVersion: v1
kind: Pod
metadata:
  name: static-web1
spec:
  containers:
    - name: web
      image: nginx
      env:
      - name: USER
        valueFrom:
          configMapKeyRef:
            name: newconfig1
            key: user

kubectl get cm 
kubectl get cm newconfig1 -o yaml
kubectl apply -f pod.yaml
kubectl exec -it static-web1 -- bash
echo $USER


SECRETS:
To store sensitive data in an unencrypted format like passwords, ssh-keys etc ---
it uses base64 encoded format
Designed to prevent exposure or unauthorized access to sensitive information within the cluster.
Both secrets and Config maps are used outside of deployment
config maps cannot encode but secrets will encode the data
password=raham (now we can encode and ecode the value)

WHY: 
if i dont want to expose the sensitive info so we use SECRETS
By default k8s will create some Secrets these are useful from me to create communicate inside the cluster
used to communicate with one resoure to another in cluster
These are system created secrets, we need not to delete

TYPES OF SECRETS:
K8s supports different types of secrets:
Generic: Generic secrets allow you to store arbitrary key-value pairs.
TLS: To store TLS certificates for securing communication between services.
Docker-registry: Docker-registry secrets are used for storing credentials required to authenticate with private Docker registries.
SSH: To store SSH private keys, used for secure access to external resources.



kubectl get secret
kubectl create secrets -A
kubectl create secrets generic -h
kubectl create secret generic password --from-literal=ROOT_PASSWORD=raham123
kubectl get secrets
kubectl get secrets password -o yaml
password is encoded so we are now decoding it to get the original info
echo "cmFoYW0xMjM=" | base64 --decode
echo "cmFoYW0xMjM=" | base64 -d
it can only encode and decode data but it cannot encrypt the data if you want encrypt the data we need to use
ansible vault or terraform vault etc ---
Best thing is we never expose directly
in real time we cannot give secrets access to all persons 
we can give it specific people by using RBAC or Service accounts


kubectl create deploy newdb --image=mariadb
kubectl get pods
kubectl logs newdb-58b55d5ddd-24pgv
kubectl set env deploy newdb --from=secret/password
kubectl get po  #it will fail now  
kubectl describe pod newdb-58b55d5ddd-24pgv
if i dont give prefix it will go to crashloop backoff
kubectl set env deploy newdb --from=secret/password --prefix=MYSQL_
 kubectl edit deploy newdb
you can see ROOT_PASSWORD it doesn't have any value we can ignore that as well
if you delete it(ROOT_PASSWORD) can create a new pod immediately
kubectl exec -it newdb-85f447554f-wgfvs -- env

================================================================================

HELM:
In K8S Helm is a package manager to install packages
in Redhat: yum & Ubuntu: apt & K8s: helm 
it is used to install applications on clusters.
we can install and deploy applications by using helm
it manages k8s resources packages through chats 
chart is a collection of files organized on a directory structure.
chart is collection of manifest files.
a running instance of a chart with a specific config is called a release


ADVANTAGES:
Install software.
Automatically install software dependencies.
Upgrade software.
Configure software deployments.
Fetch software packages from repositories.

COMPONETS:
The Helm Client is a command-line client for end users responsible:
Local chart development
Managing repositories & Managing releases
Interfacing with the Helm library
Sending charts to be installed
Requesting upgrading or uninstalling of existing releases.

The Helm Library provides the logic for executing all Helm operations. 
It interfaces with the Kubernetes API server and provides the following:
Combining a chart and configuration to build a release
Installing charts into Kubernetes, and providing the subsequent release object
Upgrading and uninstalling charts by interacting with Kubernetes
The standalone Helm library encapsulates the Helm logic so that it can be leveraged by different clients.

IMPLEMENATION:
Helm client and library is written in the Go programming language.
The library uses the Kubernetes client library to communicate with K8's.
 Currently, that library uses REST+JSON. 
It stores information in Secrets located inside of Kubernetes. 
It does not need its own database.
Configuration files are, when possible, written in YAML.



STEPS TO SETUP PROMETHEUS & GRAFANA IN KOPS:
INSTALL HEML:
curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh
helm version

INSTALL K8S METRICS SERVER:
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
Verify that the metrics-server deployment is running the desired number of pods 
kubectl get pods -n kube-system
kubectl get deployment metrics-server -n kube-system

INSTALL PROMETHEUS:
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts

UPDATE HELM CHART REPOS:
helm repo update
helm repo list

CREATE PROMETHEUS NAMESPACE:
kubectl create namespace prometheus
kubectl get ns

INSTALL PROMETHEUS:
helm install prometheus prometheus-community/prometheus --namespace prometheus --set alertmanager.persistentVolume.storageClass="gp2" --set server.persistentVolume.storageClass="gp2"
kubectl get pods -n prometheus
kubectl get all -n prometheus

INSTALL GRAFANA:
helm repo add grafana https://grafana.github.io/helm-charts

CREATE GRAFANA NAMESPACE:
kubectl create namespace grafana


helm install grafana grafana/grafana --namespace grafana --set persistence.storageClassName="gp2" --set persistence.enabled=true --set adminPassword='RAHAM123' --set  service.type=LoadBalancer
kubectl get pods -n grafana
kubectl get service -n grafana

Copy the EXTERNAL-IP and paste in browser


add the below url in Connection and save and test
http://prometheus-server.prometheus.svc.cluster.local/

Go to Grafana Dashboard → Add the Datasource → Select the Prometheus

Import Grafana dashboard from Grafana Labs
grafana dashboard → new → Import → 6417 → load → select prometheus → import


NOW DEPLOY ANY APPLICATION AND SEE THE RESULT IN DASHBOARD.


ADD 315 PORT TO MONITOR THE FOLLOWING TERMS:
Network I/O pressure.
Cluster CPU usage.
Cluster Memory usage.
Cluster filesystem usage.
Pods CPU usage.

ADD 1860 PORT TO MONITOR NODES INDIVIDUALLY 

315
6417

11454 -- > for pv and pvcs
747 -- > pod metrics
14623 -- > k8s overview db

LOKI & PROMTAL:

wget https://raw.githubusercontent.com/grafana/loki/v2.8.0/cmd/loki/loki-local-config.yaml -O loki-config.yaml
docker run -itd --name loki -v /root/:/mnt/config -p 3100:3100 grafana/loki:2.8.0
pblicip:3100/ready

wget https://raw.githubusercontent.com/grafana/loki/v2.8.0/clients/cmd/promtail/promtail-docker-config.yaml -O promtail-config.yaml
docker run -itd --name promtail -v $(pwd):/mnt/config -v /var/log:/var/log --link loki grafana/promtail:2.8.0 --config.file=/mnt/config/promtail-config.yaml

datasource -- > add -- > loki -- > http://52.90.35.175:3100 -- > save and test
explore view


====================================================================

GITOPS:

GitOps is a way of managing software infrastructure and deployments using Git as the source of truth.

Git as the Source of Truth: In GitOps, all our configurations like (deployments, services, secrets etc..) are stored in git repository.

Automated Processes: Whenever we make any changes in those YAML files gitops tools like (Argo CD/ Flux) will detects and apply those changes on kubernetes cluster. It ensures that the live infrastructure matches the configurations in the Git repository.

Here, we can clearly observe that continuous deployment. Whenever we make any changes in git, it will automatically reflects in kubernetes cluster.

WITHOUT ARGO CD:
Before ARGO CD, we deployed applications manually by installing some third party tools like kubectl, helm etc... 

If we are working with KOPS, we need to provide our configuration details (RBAC) or If we are working on EKS, we need to provide our IAM credentials. 

If we deploy any application, there is no GUI to see the status of the deployment. 

so we are facing some security challenges and need to install some third party tools.

IMPORTANT POINTS:

Once if we implement ArgoCD, if we make any changes manually in our cluster using the kubectl command, Kubernetes will reject those request from that user. Because when we apply changes manually, ArgoCD will check the actual state of the cluster with the desired state of the cluster (GitHub).
If we make any changes in the GitHub like increasing the replicas in deployment ArgoCD will take the changes and applies in our cluster. So that we can track each and every change and it will maintain the history.
We can easily rollback using git if something went wrong.
If our entire cluster gets deleted due to some network or other issues, we don’t need to worry about it, because all our configuration files are safely stored in GitHub. So we can easily re-apply those configuration files.


FEATURES:

Automated deployment of applications to specified target environments
Ability to manage and deploy to multiple clusters
Multi-tenancy and RBAC policies for authorization
Rollback/Roll-anywhere to any application configuration committed in the Git repository
Health status analysis of application resources
Automated configuration drift detection and visualization
Automated or manual syncing of applications to its desired state
Web UI which provides a real-time view of application activity
CLI for automation and CI integration
Webhook integration (GitHub, BitBucket, GitLab)
Access tokens for automation
PreSync, Sync, PostSync hooks to support complex application rollouts (e.g.blue/green & canary upgrades)
Audit trails for application events and API calls
Prometheus metrics
Parameter overrides for overriding helm parameters in Git.

INSTALL HELM:
curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 
chmod 700 get_helm.sh
./get_helm.sh
helm version

INSTALL ARGO CD USING HELM
helm repo add argo-cd https://argoproj.github.io/argo-helm
helm repo update
kubectl create namespace argocd
helm install argocd argo-cd/argo-cd -n argocd
kubectl get all -n argocd



EXPOSE ARGOCD SERVER:
kubectl patch svc argocd-server -n argocd -p '{"spec": {"type": "LoadBalancer"}}'
yum install jq -y
export ARGOCD_SERVER='kubectl get svc argocd-server -n argocd -o json | jq --raw-output '.status.loadBalancer.ingress[0].hostname''
echo $ARGOCD_SERVER
kubectl get svc argocd-server -n argocd -o json | jq --raw-output .status.loadBalancer.ingress[0].hostname
The above command will provide load balancer URL to access ARGO CD


TO GET ARGO CD PASSWORD:
export ARGO_PWD='kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d'
echo $ARGO_PWD
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d
The above command to provide password to access argo cd


MANIFEST: https://github.com/devops0014/manifest.git

INGRESS IN K8S:
Ingress helps to expose the HTTP and HTTPS routes from outside of the cluster.
Ingress supports 
Path-based  
Host-based routing
Ingress supports Load balancing and SSL termination.
It redirects the incoming requests to the right services based on the Web URL or path in the address.
Ingress provides the encryption feature and helps to balance the load of the applications.

Ingress is used to manage the external traffic to the services within the cluster which provides features like host-based routing, path-based routing, SSL termination, and more. Where a Load balancer is used to manage the traffic but the load balancer does not provide the fine-grained access control like Ingress.

Example:
Suppose you have multiple Kubernetes services running on your cluster and each service serves a different application such as example.com/app1 and example.com/app2. With the help of Ingress, you can achieve this. However, the Load Balancer routes the traffic based on the ports and can't handle the URL-based routing.

To install ingress, firstly we have to install nginx ingress controller:
command: kubectl create -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.2.1/deploy/static/provider/cloud/deploy.yaml
Once we install ingress controller, we have to deploy 2 applications. 
github url: https://github.com/mustafaprojectsindevops/kubernetes/tree/master/ingress
After executing all the files, use kubectl get ing to get ingress. After 30 seconds it will provide one load balancer dns.

access those applications using dns/nginx and dns/httpd. So the traffic will route into both the applications as per the routing

=====================================================================================


